import numpy as np
import pandas as pd
import sklearn
import string
import re
from textblob import TextBlob
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('brown')
nltk.download('averaged_perceptron_tagger')
from nltk.probability import FreqDist
from sklearn.decomposition import LatentDirichletAllocation,NMF
#import spacy
#from wordcloud import WordCloud,STOPWORDS
from collections import Counter,defaultdict
#from PIL import Image
#from transformers import BertTokenizer,BertForSequenceClassification,AdamW,BertConfig,get_linear_schedule_with_warmup
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import SnowballStemmer
from nltk.stem import LancasterStemmer
from nltk.corpus import wordnet
from nltk.corpus import brown
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import gensim
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import torch
from torchtext import data
import torch.nn as nn
#from torch.utils.data import DataLoader,TensorDataset
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from collections import Counter
import seaborn as sns
import datetime
from sklearn.metrics import accuracy_score,f1_score
import contractions
from torch.utils.data import DataLoader,TensorDataset
import time
from nltk.corpus import stopwords
#device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
train_df=pd.read_csv(r'/home/pegah/Desktop/train_NN.csv')
print(train_df.info())
print(train_df.head(7))
test=pd.read_csv(r'/home/pegah/Desktop/test_NN.csv')
print(test.info())
#train_df.drop(columns=['id','keyword','location'],inplace=True)
#print(train_df[~train_df["location"].isnull()].head(7))
#print(train_df[train_df["target"]==0]["text"].values[1])
#print(train_df[train_df["target"]==1]["text"].values[1])
#text cleaning
train_df["text_clean"]=train_df["text"].apply(lambda x:x.lower())
print(train_df.head(7))
train_df["text_clean"]=train_df["text_clean"].apply(lambda x:contractions.fix(x))
print(train_df["text"][67])
print(train_df["text_clean"][67])
def remove_URL(text):
    return re.sub(r"http?://\S+|www\.\S+","",text)
print(train_df["text"][31])
print(train_df["text_clean"][31])
print(train_df["text"][37])
print(train_df["text_clean"][37])
def remove_html(text):
    html=re.compile(r"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});")
    return re.sub(html,"",text)
train_df["text_clean"]=train_df["text_clean"].apply(lambda x:remove_html(x))
print(train_df["text"][62])
print(train_df["text_clean"][62])
def remove_non_ascii(text):
    return re.sub(r'[^\x00-\x7f]',r'',text)
train_df["text_clean"]=train_df["text_clean"].apply(lambda x:remove_non_ascii(x))
print(train_df["text"][7586])
print(train_df["text_clean"][7586])
def remove_punct(text):
    return text.translate(str.maketrans('','',string.punctuation))
train_df["text_clean"]=train_df["text_clean"].apply(lambda x:remove_punct(x))
print(train_df["text"][5])
print(train_df["text_clean"][5])
def other_clean(text):
    sample_typos_slang = { "w/e": "whatever", "usagov": "usa government", "recentlu": "recently", "ph0tos": "photos", "amirite": "am i right", "exp0sed": "exposed", "<3": "love", "luv": "love", "amageddon": "armageddon", "trfc": "traffic", "16yr": "16 year" }
    sample_acronyms = { "mh370": "malaysia airlines flight 370", "okwx": "oklahoma city weather", "arwx": "arkansas weather", "gawx": "georgia weather", "scwx": "south carolina weather", "cawx": "california weather", "tnwx": "tennessee weather", "azwx": "arizona weather", "alwx": "alabama weather", "usnwsgov": "united states national weather service", "2mw": "tomorrow" }
    sample_abbr = { "$" : " dollar ", "€" : " euro ", "4ao" : "for adults only", "a.m" : "before midday", "a3" : "anytime anywhere anyplace", "aamof" : "as a matter of fact", "acct" : "account", "adih" : "another day in hell", "afaic" : "as far as i am concerned", "afaict" : "as far as i can tell", "afaik" : "as far as i know", "afair" : "as far as i remember", "afk" : "away from keyboard", "app" : "application", "approx" : "approximately", "apps" : "applications", "asap" : "as soon as possible", "asl" : "age, sex, location", "atk" : "at the keyboard", "ave." : "avenue", "aymm" : "are you my mother", "ayor" : "at your own risk", "b&b" : "bed and breakfast", "b+b" : "bed and breakfast", "b.c" : "before christ", "b2b" : "business to business", "b2c" : "business to customer", "b4" : "before", "b4n" : "bye for now", "b@u" : "back at you", "bae" : "before anyone else", "bak" : "back at keyboard", "bbbg" : "bye bye be good", "bbc" : "british broadcasting corporation", "bbias" : "be back in a second", "bbl" : "be back later", "bbs" : "be back soon", "be4" : "before", "bfn" : "bye for now", "blvd" : "boulevard", "bout" : "about", "brb" : "be right back", "bros" : "brothers", "brt" : "be right there", "bsaaw" : "big smile and a wink", "btw" : "by the way", "bwl" : "bursting with laughter", "c/o" : "care of", "cet" : "central european time", "cf" : "compare", "cia" : "central intelligence agency", "csl" : "can not stop laughing", "cu" : "see you", "cul8r" : "see you later", "cv" : "curriculum vitae", "cwot" : "complete waste of time", "cya" : "see you", "cyt" : "see you tomorrow", "dae" : "does anyone else", "dbmib" : "do not bother me i am busy", "diy" : "do it yourself", "dm" : "direct message", "dwh" : "during work hours", "e123" : "easy as one two three", "eet" : "eastern european time", "eg" : "example", "embm" : "early morning business meeting", "encl" : "enclosed", "encl." : "enclosed", "etc" : "and so on", "faq" : "frequently asked questions", "fawc" : "for anyone who cares", "fb" : "facebook", "fc" : "fingers crossed", "fig" : "figure", "fimh" : "forever in my heart", "ft." : "feet", "ft" : "featuring", "ftl" : "for the loss", "ftw" : "for the win", "fwiw" : "for what it is worth", "fyi" : "for your information", "g9" : "genius", "gahoy" : "get a hold of yourself", "gal" : "get a life", "gcse" : "general certificate of secondary education", "gfn" : "gone for now", "gg" : "good game", "gl" : "good luck", "glhf" : "good luck have fun", "gmt" : "greenwich mean time", "gmta" : "great minds think alike", "gn" : "good night", "g.o.a.t" : "greatest of all time", "goat" : "greatest of all time", "goi" : "get over it", "gps" : "global positioning system", "gr8" : "great", "gratz" : "congratulations", "gyal" : "girl", "h&c" : "hot and cold", "hp" : "horsepower", "hr" : "hour", "hrh" : "his royal highness", "ht" : "height", "ibrb" : "i will be right back", "ic" : "i see", "icq" : "i seek you", "icymi" : "in case you missed it", "idc" : "i do not care", "idgadf" : "i do not give a damn fuck", "idgaf" : "i do not give a fuck", "idk" : "i do not know", "ie" : "that is", "i.e" : "that is", "ifyp" : "i feel your pain", "IG" : "instagram", "iirc" : "if i remember correctly", "ilu" : "i love you", "ily" : "i love you", "imho" : "in my humble opinion", "imo" : "in my opinion", "imu" : "i miss you", "iow" : "in other words", "irl" : "in real life", "j4f" : "just for fun", "jic" : "just in case", "jk" : "just kidding", "jsyk" : "just so you know", "l8r" : "later", "lb" : "pound", "lbs" : "pounds", "ldr" : "long distance relationship", "lmao" : "laugh my ass off", "lmfao" : "laugh my fucking ass off", "lol" : "laughing out loud", "ltd" : "limited", "ltns" : "long time no see", "m8" : "mate", "mf" : "motherfucker", "mfs" : "motherfuckers", "mfw" : "my face when", "mofo" : "motherfucker", "mph" : "miles per hour", "mr" : "mister", "mrw" : "my reaction when", "ms" : "miss", "mte" : "my thoughts exactly", "nagi" : "not a good idea", "nbc" : "national broadcasting company", "nbd" : "not big deal", "nfs" : "not for sale", "ngl" : "not going to lie", "nhs" : "national health service", "nrn" : "no reply necessary", "nsfl" : "not safe for life", "nsfw" : "not safe for work", "nth" : "nice to have", "nvr" : "never", "nyc" : "new york city", "oc" : "original content", "og" : "original", "ohp" : "overhead projector", "oic" : "oh i see", "omdb" : "over my dead body", "omg" : "oh my god", "omw" : "on my way", "p.a" : "per annum", "p.m" : "after midday", "pm" : "prime minister", "poc" : "people of color", "pov" : "point of view", "pp" : "pages", "ppl" : "people", "prw" : "parents are watching", "ps" : "postscript", "pt" : "point", "ptb" : "please text back", "pto" : "please turn over", "qpsa" : "what happens", "ratchet" : "rude", "rbtl" : "read between the lines", "rlrt" : "real life retweet", "rofl" : "rolling on the floor laughing", "roflol" : "rolling on the floor laughing out loud", "rotflmao" : "rolling on the floor laughing my ass off", "rt" : "retweet", "ruok" : "are you ok", "sfw" : "safe for work", "sk8" : "skate", "smh" : "shake my head", "sq" : "square", "srsly" : "seriously", "ssdd" : "same stuff different day", "tbh" : "to be honest", "tbs" : "tablespooful", "tbsp" : "tablespooful", "tfw" : "that feeling when", "thks" : "thank you", "tho" : "though", "thx" : "thank you", "tia" : "thanks in advance", "til" : "today i learned", "tl;dr" : "too long i did not read", "tldr" : "too long i did not read","tmb" : "tweet me back", "tntl" : "trying not to laugh", "ttyl" : "talk to you later", "u" : "you", "u2" : "you too", "u4e" : "yours for ever", "utc" : "coordinated universal time", "w/" : "with", "w/o" : "without", "w8" : "wait", "wassup" : "what is up", "wb" : "welcome back", "wtf" : "what the fuck", "wtg" : "way to go", "wtpa" : "where the party at", "wuf" : "where are you from", "wuzup" : "what is up", "wywh" : "wish you were here", "yd" : "yard", "ygtr" : "you got that right", "ynk" : "you never know", "zzz" : "sleeping bored and tired" }
    sample_typos_slang_pattern = re.compile(r'(?<!\w)(' + '|'.join(re.escape(key) for key in sample_typos_slang.keys()) + r')(?!\w)')
    sample_acronyms_pattern = re.compile(r'(?<!\w)(' + '|'.join(re.escape(key) for key in sample_acronyms.keys()) + r')(?!\w)')
    sample_abbr_pattern = re.compile(r'(?<!\w)(' + '|'.join(re.escape(key) for key in sample_abbr.keys()) + r')(?!\w)')
    text = sample_typos_slang_pattern.sub(lambda x: sample_typos_slang[x.group()], text)
    text = sample_acronyms_pattern.sub(lambda x: sample_acronyms[x.group()], text)
    text = sample_abbr_pattern.sub(lambda x: sample_abbr[x.group()], text)
    return text
train_df["text_clean"]=train_df["text_clean"].apply(lambda x:other_clean(x))
print(train_df["text"][4409])
print(train_df["text_clean"][4409])
print("text:",TextBlob("sleapy and there is no plaxe Im gioong to.").correct())
train_df['tokenized']=train_df['text_clean'].apply(word_tokenize)
print(train_df.head(7))
stop=set(stopwords.words('english'))
train_df['stopwords_removed']=train_df['tokenized'].apply(lambda x:[word for word in x if word not in stop])
print(train_df.head(7))
def porter_stemmer(text):
    stemmer=nltk.PorterStemmer()
    stems=[stemmer.stem(i) for i in text]
    return stems
train_df['porter_stemmer']=train_df['stopwords_removed'].apply(lambda x:porter_stemmer(x))
print(train_df.head(7))
def snowball_stemmer(text):
    stemmer=nltk.SnowballStemmer("english")
    stems=[stemmer.stem(i) for i in text]
    return stems
train_df['snowball_stemmer']=train_df['stopwords_removed'].apply(lambda x:snowball_stemmer(x))
print(train_df.head(7))
def lancaster_stemmer(text):
    stemmer=nltk.LancasterStemmer()
    stems=[stemmer.stem(i) for i in text]
    return stems
train_df['lancaster_stemmer']=train_df['stopwords_removed'].apply(lambda x:lancaster_stemmer(x))
print(train_df.head(7))
wordnet_map={"N":wordnet.NOUN,"V":wordnet.VERB,"J":wordnet.ADJ,"R":wordnet.ADV}
train_sents=brown.tagged_sents(categories='news')
t0=nltk.DefaultTagger('NN')
t1=nltk.UnigramTagger(train_sents,backoff=t0)
t2=nltk.BigramTagger(train_sents,backoff=t1)
def pos_tag_wordnet(text,pos_tag_type="pos_tag"):
    pos_tagged_text=t2.tag(text)
    pos_tagged_text=[(word,wordnet_map.get(pos_tag[0])) if pos_tag[0] in wordnet_map.keys() else (word,wordnet.NOUN) for (word,pos_tag) in pos_tagged_text]
    return pos_tagged_text
pos_tag_wordnet(train_df['stopwords_removed'][2])
train_df['combined_postag_wnet']=train_df['stopwords_removed'].apply(lambda x:pos_tag_wordnet(x))
print(train_df.head(7))
def lemmatize_word(text):
    lemmatizer=WordNetLemmatizer()
    lemma=[lemmatizer.lemmatize(word,tag) for word,tag in text]
    return lemma
#lemmatization without pos tagging
lemmatizer=WordNetLemmatizer()
train_df['lemmatize_word_wo_pos']=train_df['stopwords_removed'].apply(lambda x:[lemmatizer.lemmatize(word) for word in x])
train_df['lemmatize_word_wo_pos']=train_df['lemmatize_word_wo_pos'].apply(lambda x:[word for word in x if word not in stop])
print(train_df.head(7))
print(train_df["combined_postag_wnet"][8])
print(train_df["lemmatize_word_wo_pos"][8])
#lemmatization with pos tag
lemmatizer=WordNetLemmatizer()
train_df['lemmatize_word_wo_pos']=train_df['combined_postag_wnet'].apply(lambda x:lemmatize_word(x))
train_df['lemmatize_word_wo_pos']=train_df['lemmatize_word_wo_pos'].apply(lambda x:[word for word in x if word not in stop])
train_df['lemmatize_text']=[' '.join(map(str,l)) for l in train_df['lemmatize_word_wo_pos']]
print(train_df.head(7))
print(train_df["text"][8])
print(train_df["combined_postag_wnet"][8])
print(train_df["lemmatize_word_wo_pos"][8])
#print(train_df["lemmatize_word_w_pos"][8])
print(train_df["text"][0],train_df["lemmatize_text"][0])
def cv(data,ngram=1,MAX_NB_WORDS=75000):
    count_vectorizer=CountVectorizer(ngram_range=(ngram,ngram),max_features=MAX_NB_WORDS)
    emb=count_vectorizer.fit_transform(data).toarray()
    print("count vectorize with",str(np.array(emb).shape[1]),"features")
    return emb,count_vectorizer
def print_out(emb,feat,ngram,compared_sentence=0):
    print(ngram,"bag_of_word:")
    #print(feat.get_features_names(),"\n")
    print(ngram,"bag_of_feature:")
    print(test_cv_1gram.vocabulary_,"\n")
    print("bow matrix:")
    print(pd.DataFrame(emb.transpose(),index=feat.get_feature_names()).head(7),"\n")
    print(ngram,"vector example:")
    print(train_df["lemmatize_text"][compared_sentence])
    print(emb[compared_sentence],"\n")
test_corpus=train_df["lemmatize_text"][:5].tolist()
print("the test corpus:",test_corpus,"\n")
test_cv_em_1gram,test_cv_1gram=cv(test_corpus,ngram=1)
print_out(test_cv_em_1gram,test_cv_1gram,ngram="Uni-gram")
test_cv_em_2gram,test_cv_2gram=cv(test_corpus,ngram=2)
print_out(test_cv_em_2gram,test_cv_2gram,ngram="Bi-gram")
test_cv_em_3gram,test_cv_3gram=cv(test_corpus,ngram=3)
print_out(test_cv_em_3gram,test_cv_3gram,ngram="Tri-gram")
train_df_corpus=train_df["lemmatize_text"].tolist()
train_df_em_1gram,vc_1gram=cv(train_df_corpus,1)
train_df_em_2gram,vc_2gram=cv(train_df_corpus,2)
train_df_em_3gram,vc_3gram=cv(train_df_corpus,3)
print(len(train_df_corpus))
print(train_df_em_1gram.shape)
print(train_df_em_2gram.shape)
print(train_df_em_3gram.shape)
del train_df_em_1gram,train_df_em_2gram,train_df_em_3gram
def TFIDF(data,ngram=1,MAX_NB_WORDS=75000):
    tfidf_x=TfidfVectorizer(ngram_range=(ngram,ngram),max_features=MAX_NB_WORDS)
    emb=tfidf_x.fit_transform(data).toarray()
    print("tf-idf with",str(np.array(emb).shape[1]),"features")
    return emb,tfidf_x
test_corpus=train_df["lemmatize_text"][:5].tolist()
print("the test corpus:",test_corpus,"\n")
test_tfidf_em_1gram,test_tfidf_1gram=TFIDF(test_corpus,ngram=1)
print_out(test_tfidf_em_1gram,test_tfidf_1gram,ngram="Uni-gram")
test_tfidf_em_2gram,test_tfidf_2gram=TFIDF(test_corpus,ngram=2)
print_out(test_tfidf_em_2gram,test_tfidf_2gram,ngram="Bi-gram")
test_tfidf_em_3gram,test_tfidf_3gram=TFIDF(test_corpus,ngram=3)
print_out(test_tfidf_em_3gram,test_tfidf_3gram,ngram="Tri-gram")
train_df_corpus=train_df["lemmatize_text"].tolist()
train_df_tfidf_1gram,tfidf_1gram=TFIDF(train_df_corpus,1)
train_df_tfidf_2gram,tfidf_2gram=TFIDF(train_df_corpus,2)
train_df_tfidf_3gram,tfidf_3gram=TFIDF(train_df_corpus,3)
print(len(train_df_corpus))
print(train_df_tfidf_1gram.shape)
print(train_df_tfidf_2gram.shape)
print(train_df_tfidf_3gram.shape)
del train_df_tfidf_1gram,train_df_tfidf_2gram,train_df_tfidf_3gram
train_df_id=train_df['id']
train_df.drop(['id'],axis=1,inplace=True)
test_id=test['id']
test.drop(['id'],axis=1,inplace=True)
word2idx={}
new_embedding_index={}
train_X_list=[]
index=1
embeddings_index={}
with open('kaggle/input/glove100d/glove.6B.50d.txt','r',encoding='utf8') as f:
    for line in f:
        values=line.split()
        word=values[0]
        coefs=np.array(values[1:]).astype(np.float)
        embeddings_index[word]=coefs
print('found %s word vector'%len(embeddings_index))
embed_keys=embeddings_index.keys()
for x in train_df['text']:
    list1=x.split(' ')
    new_list=[]
    for i in list1:
        if((i in embed_keys) and (i not in word2idx.keys())):
            new_embedding_index[index]=embeddings_index[i]
            word2idx[i]=index
            new_list.append(index)
            index=index+1
        elif( i not in word2idx.keys()):
              new_embedding_index[index]=np.random.normal(scale=0.4,sixe=(50, )).astype(np.float)
              word2idx[i]=index
              new_list.append(index)
              index=index+1
        else:
            new_list.append(word2idx[i])
            train_X_list.append(new_list)
test_X_list=[]
index=len(word2idx)+1
embed_keys=embeddings_index.keys()
for x in test['text']:
    list1=x.split(' ')
    new_list=[]
    for i in list1:
        if((i in embed_keys) and (i not in word2idx.keys())):
            new_embedding_index[index]=embeddings_index[i]
            word2idx[i]=index
            new_list.append(index)
            index=index+1
        elif( i not in word2idx.keys()):
              new_embedding_index[index]=np.random.normal(scale=0.4,sixe=(50, )).astype(np.float)
              word2idx[i]=index
              new_list.append(index)
              index=index+1
        else:
            new_list.append(word2idx[i])
            test_X_list.append(new_list)
print(len(new_embedding_index))
max(map(len,train_X_list))
max(map(len,test_X_list))
def pad_features(reviews_int,seq_length):
    features=np.zeros((len(reviews_int),seq_length),dtype=int)
    for i , review in enumerate(reviews_int):
        review_len=len(review)
        if review_len<=seq_length:
            zeros=list(np.zeros(seq_length-review_len))
            new=zeros+review
        elif review_len>seq_length:
            new=review[0:seq_length]
            features[i,:]=np.array(new)
    return features
train_X_list=pad_features(train_X_list,55)
for i in range(3):
    extra_list=[np.array(np.zeros(55).astype(int))]
    train_X_list=np.append(train_X_list,extra_list,axis=0)
print(len(train_X_list))
train_y_list=[]
for i in train_df['target']:
    train_y_list.append(i)
for i in range(3):
    train_y_list.append(0)
print(len(train_y_list))
train_y_list=np.array(train_y_list)
test_X_list=pad_features(test_X_list,55)
extra_list=[np.array(np.zeros(55).astype(int))]
test_X_list=np.append(test_X_list,extra_list,axis=0)
new_embedding_index[0]=np.array(np.zeros(50)).astype(np.float)
train_data=TensorDataset(torch.from_numpy(train_X_list),torch.from_numpy(train_y_list))
batch_size=16
train_loader=DataLoader(train_data,batch_size=batch_size,drop_last=True)
class LSTM_net(nn.Module):
    def __init__(self,weights_matrix,output_size,hidden_dim,hidden_dim2,n_layers,drop_prob=0.5):
        super(LSTM_net,self).__init__()
        self.output_size=output_size
        self.n_layers=n_layers
        self.hidden_dim=hidden_dim
        num_embeddings,embedding_dim=weights_matrix.shape
        self.embedding=nn.Embedding(num_embeddings,embedding_dim)
        self.embedding.weigth=nn.Parameter(weights_matrix)
        self.lstm=nn.LSTM(embedding_dim,hidden_dim,n_layers,dropout=drop_prob,bidirectional=True,batch_first=True)
        self.dropout=nn.Dropout(0.3)
        self.fullyconnect1=nn.Linear(hidden_dim,hidden_dim2)
        self.fullyconnect2=nn.Linear(hidden_dim2,output_size)
        self.sig=nn.Sigmoid()
    def forward(self,x,hidden):
        batch_size=x.size(0)
        embeds=self.embedding(x)
        lstm_outs,hidden=self.lstm(embeds,hidden)
        lstm_outs=lstm_outs.contiguous().view(-1,self.hidden_dim)
        out=self.dropout(lstm_outs)
        out=self.fullyconnect1(out)
        out=self.dropout(out)
        out=self.fullyconnect2(out)
        sig_out=self.sig(out)
        sig_out=sig_out.view(batch_size,-1)
        sig_out=sig_out[:,-1]
        return sig_out,hidden
    def init_hidden(self,batch_size,train_on_gpu=False):
        weight=next(self.parameters()).data
        if (train_on_gpu):
            hidden=(weight.new(self.n_layers*2,batch_size,self.hidden_dim).zero_().cuda(),weight.new(self.n_layers*2,batch_size,self.hidden_dim).zero_().cuda())

        else:
            hidden=(weight.new(self.n_layers*2,batch_size,self.hidden_dim).zero_(),weight.new(self.n_layers*2,batch_size,self.hidden_dim).zero_())
        return hidden
vals=np.array(list(new_embedding_index.values()))
vals=torch.from_numpy(vals)
output_size=1
hidden_dim=200
hidden_dim2=50
n_layers=2
model=LSTM_net(vals,output_size,hidden_dim,hidden_dim2,n_layers)
print(model)
train_on_gpu=True
lr=0.001
criterion=nn.BCELoss()
optimizer=torch.optim.Adam(model.parameters(),lr=lr)
epochs=12
counter=0
print_every=64
clip=5
model=model.float()
if(train_on_gpu):
    model.cuda()
model.train()
for e in range(epochs):
    h=model.init_hidden(batch_size)
    for inputs,labels in train_loader:
        counter+=1
        h=tuple([each.data for each in h])
        model.zero_grad()
        inputs=inputs.type(torch.LongTensor)
        inputs=inputs.cuda()
        labels=labels.cuda()
        output,h=model(inputs,h)
        loss=criterion(output.squeeze(),labels.float())
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(),clip)
        optimizer.step()
    print("epoch:".format(e+1,epochs,"loss:".format(loss.item())))

test_data=torch.from_numpy(test_X_list)
test_loader=DataLoader(test_data,batch_size=batch_size)
h=model.init_hidden(batch_size)
pred=[]
model.eval()
for inputs in test_loader:
    h=tuple([each.data for each in h])
    inputs=inputs.type(torch.LongTensor)
    if(train_on_gpu):
        inputs=inputs.cuda()
    output,h=model(inputs,h)
    pred.append(torch.round(output.squeeze()))
prediction=[]
for i in pred:
    prediction.append(i.tolist())
pred=[]
pred=[item for sublist in prediction for item in sublist]
pred=pred[:-1]
pred=[int(i) for i in pred]
print(len(pred))
output=pd.DataFrame({'id':test_id,'target':pred})
output.sort_values(["id"],axis=0,ascending=True,inplace=True)
output.to_csv('submission.csv',index=False)
#loss=[]
#acc=[]
#val_acc=[]
#for epoch in range(num_epochs):
 #   train_loss,train_acc=train(model,train_iterator)
  # valid_acc=evaluate(model,valid_iterator)
   # print('train loss:{train_loss:.3f}|train acc:{train_acc*100:.2f}%')
    #print('val acc:{valid_acc*100:.2f}%')
    #loss.append(train_loss)
    #acc.append(train_acc)
    #val_acc.append(valid_acc)
plt.xlabel("epochs")
plt.ylabel("train loss")
#x_len=list(range(len(acc)))
#plt.axis([0,max(x_len),0,1])
plt.title('result of LSTM')
loss=np.asarray(loss)/max(loss)
#plt.plot(x_len,loss,'r.',label="loss")
#plt.plot(x_len,acc,'b.',label="accuracy")
#plt.plot(x_len,val_acc,'g.',label="val_accuracy")
#plt.legend(bbox_to_anchor=(1.05,1),loc=2,borderaxespad=0.2)
plt.show()
